{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc40a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53e0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9627fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers from Wikipedia page:\n",
      "1. Main Page\n",
      "2. Welcome to Wikipedia\n",
      "3. From today's featured article\n",
      "4. Did you know ...\n",
      "5. In the news\n",
      "6. On this day\n",
      "7. Today's featured picture\n",
      "8. Other areas of Wikipedia\n",
      "9. Wikipedia's sister projects\n",
      "10. Wikipedia languages\n",
      "\n",
      " DataFrame:\n",
      "\n",
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_wikipedia_headers(url):\n",
    "    # Send a GET request to the Wikipedia page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    # Find all header tags (h1 to h6)\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "\n",
    "    return headers\n",
    "\n",
    "def create_dataframe(headers):\n",
    "    # Create a DataFrame from the list of headers\n",
    "    df = pd.DataFrame({'Headers': headers})\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Wikipedia URL\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "    # Get headers from the Wikipedia page\n",
    "    headers = get_wikipedia_headers(wikipedia_url)\n",
    "\n",
    "    if headers:\n",
    "        # Display headers\n",
    "        print(\"Headers from Wikipedia page:\")\n",
    "        for i, header in enumerate(headers, start=1):\n",
    "            print(f\"{i}. {header}\")\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = create_dataframe(headers)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(\"\\n DataFrame:\\n\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Failed to retrieve headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe8aecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Respected Former Presidents of India:\n",
      "1. Name: Shri Ram Nath Kovind, Term of Office: 14th President of India\n",
      "2. Name: Shri Pranab Mukherjee, Term of Office: 13th President of India\n",
      "3. Name: Smt Pratibha Devisingh Patil, Term of Office: 12th President of India\n",
      "4. Name: DR. A.P.J. Abdul Kalam, Term of Office: 11th President of India\n",
      "5. Name: Shri K. R. Narayanan, Term of Office: 10th President of India\n",
      "6. Name: Dr Shankar Dayal Sharma, Term of Office: 9th  President of India\n",
      "7. Name: Shri R Venkataraman, Term of Office: 8th President of India\n",
      "8. Name: Giani Zail Singh, Term of Office: 7th President of India\n",
      "9. Name: Shri Neelam Sanjiva Reddy, Term of Office: 6th President of India\n",
      "10. Name: Dr. Fakhruddin Ali Ahmed, Term of Office: 5th President of India\n",
      "11. Name: Shri Varahagiri Venkata Giri, Term of Office: 4th President of India\n",
      "12. Name: Dr. Zakir Husain, Term of Office: 3rd President of India\n",
      "13. Name: Dr. Sarvepalli Radhakrishnan, Term of Office: 2nd President of India\n",
      "14. Name: Dr. Rajendra Prasad, Term of Office: 1st President of India\n",
      "\n",
      "DataFrame:\n",
      "                            Name           Term of Office\n",
      "0           Shri Ram Nath Kovind  14th President of India\n",
      "1          Shri Pranab Mukherjee  13th President of India\n",
      "2   Smt Pratibha Devisingh Patil  12th President of India\n",
      "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
      "4           Shri K. R. Narayanan  10th President of India\n",
      "5        Dr Shankar Dayal Sharma  9th  President of India\n",
      "6            Shri R Venkataraman   8th President of India\n",
      "7               Giani Zail Singh   7th President of India\n",
      "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
      "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
      "10  Shri Varahagiri Venkata Giri   4th President of India\n",
      "11              Dr. Zakir Husain   3rd President of India\n",
      "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
      "13           Dr. Rajendra Prasad   1st President of India\n"
     ]
    }
   ],
   "source": [
    "# 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "# from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_former_presidents(url):\n",
    "    # Send a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    \n",
    "\n",
    "#     # Find the container with the information about former presidents\n",
    "#     container = soup.find_all('div', class_='president-listing')\n",
    "    \n",
    "    if not soup:\n",
    "        print(\"Unable to find information about former presidents.\")\n",
    "        return None\n",
    "\n",
    "    # Extract information about former presidents\n",
    "    presidents_data = []# Empty list\n",
    "    for president_div in soup.find_all('div', class_='president-listing'):\n",
    "        name = president_div.find('h3').text.strip()\n",
    "        term_of_office = president_div.find('h5').text.strip()\n",
    "        presidents_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "\n",
    "    return presidents_data\n",
    "\n",
    "def create_dataframe(data):\n",
    "    # Create a DataFrame from the list of presidents' data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # URL of the page with information about former presidents\n",
    "    presidents_url = \"https://presidentofindia.nic.in/former-presidents\"\n",
    "\n",
    "    # Get data about former presidents from the specified URL\n",
    "    presidents_data = get_former_presidents(presidents_url)\n",
    "\n",
    "    if presidents_data:\n",
    "        # Display data\n",
    "        print(\"List of Respected Former Presidents of India:\")\n",
    "        for i, president in enumerate(presidents_data, start=1):\n",
    "            print(f\"{i}. Name: {president['Name']}, Term of Office: {president['Term of Office']}\")\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = create_dataframe(presidents_data)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(\"\\nDataFrame:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Failed to retrieve information about former presidents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "328a75f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 29 (3225324310.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[64], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    return teams_data\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 29\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_rankings(url):\n",
    "    # Send a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    print(soup)\n",
    "\n",
    "    if not soup:\n",
    "        print(\"Unable to find the ODI team rankings table.\")\n",
    "        return None\n",
    "\n",
    "    # Extract data for the top 10 ODI teams\n",
    "    teams_data = []\n",
    "#     for row in soup.find('si-table-body').find_all('si-table-row')[:10]:\n",
    "#         columns = row.find_all('td')\n",
    "#         team_name = columns[1].text.strip()\n",
    "#         matches = int(columns[2].text.strip())\n",
    "#         points = int(columns[3].text.strip())\n",
    "#         rating = int(columns[4].text.strip())\n",
    "#         teams_data.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "    for row in soup.find_all('div', class_='si-table-row')[:10]:\n",
    "#         name = president_div.find('h3').text.strip()\n",
    "#         term_of_office = president_div.find('h5').text.strip()\n",
    "#         presidents_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "\n",
    "    return teams_data\n",
    "\n",
    "def create_dataframe(data):\n",
    "    # Create a DataFrame from the list of teams' data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # URL of the ODI team rankings page on ICC's website\n",
    "    odi_rankings_url = \"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\"\n",
    "\n",
    "    # Get data for the top 10 ODI teams\n",
    "    teams_data = scrape_odi_rankings(odi_rankings_url)\n",
    "    if teams_data:\n",
    "        # Display data\n",
    "        print(\"Top 10 ODI Teams in Men's Cricket:\")\n",
    "        for i, team in enumerate(teams_data, start=1):\n",
    "            print(f\"{i}. Team: {team['Team']}, Matches: {team['Matches']}, Points: {team['Points']}, Rating: {team['Rating']}\")\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = create_dataframe(teams_data)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(\"\\nDataFrame:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Failed to retrieve ODI team rankings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11f24c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 1: Exclusive content, priority ticket access and personalised news. Never miss a moment!Sign inHomeRankingsFixtures & ResultsNewsEventsU19 Cricket World CupCricket world cupAwardsVideosTeams100% CricketCrictosHall of FameCriiioAbout ICCMedia ReleasesPhotosCommercial OpportunitiesShopTravelENGLISHRankingsNewsAwardsWatchSearchaccountlogout\n",
      "Element 2: overviewabout rankingsplayer rankings faqs\n",
      "Element 3: \n",
      "Element 4: \n",
      "Element 5: Terms of ServicePrivacy PolicyCareersRelated SitesaboutCopyright  2024 ICC. All rights reserved.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m odi_rankings_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/mens/team-rankings/odi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Get data for the top 10 ODI teams\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m teams_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_odi_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[43modi_rankings_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m teams_data:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Display data\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 ODI Teams in Men\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Cricket:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[85], line 29\u001b[0m, in \u001b[0;36mscrape_odi_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Extract data for the top 10 ODI teams\u001b[39;00m\n\u001b[0;32m     28\u001b[0m teams_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m10\u001b[39m]:\n\u001b[0;32m     30\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m     team_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_rankings(url):\n",
    "    # Send a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content)\n",
    "#     print(soup)\n",
    "#     # Find the table containing ODI team rankings\n",
    "#     table = soup.find('table', {'class': 'si-table'}) \n",
    "    tables = soup.find_all('div', class_='container')\n",
    "\n",
    "    # Iterate through the child elements\n",
    "    for index, child_element in enumerate(tables, start=1):\n",
    "        print(f\"Element {index}: {child_element.text.strip()}\")\n",
    "\n",
    "    if not tables:\n",
    "        print(\"Unable to find the ODI team rankings table.\")\n",
    "        return None\n",
    "\n",
    "    # Extract data for the top 10 ODI teams\n",
    "    teams_data = []\n",
    "    for row in table.find('tbody').find_all('tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        team_name = columns[1].text.strip()\n",
    "        matches = int(columns[2].text.strip())\n",
    "        points = int(columns[3].text.strip())\n",
    "        rating = int(columns[4].text.strip())\n",
    "        teams_data.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return teams_data\n",
    "\n",
    "def create_dataframe(data):\n",
    "    # Create a DataFrame from the list of teams' data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # URL of the ODI team rankings page on ICC's website\n",
    "    odi_rankings_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "\n",
    "    # Get data for the top 10 ODI teams\n",
    "    teams_data = scrape_odi_rankings(odi_rankings_url)\n",
    "\n",
    "    if teams_data:\n",
    "        # Display data\n",
    "        print(\"Top 10 ODI Teams in Men's Cricket:\")\n",
    "        for i, team in enumerate(teams_data, start=1):\n",
    "            print(f\"{i}. Team: {team['Team']}, Matches: {team['Matches']}, Points: {team['Points']}, Rating: {team['Rating']}\")\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = create_dataframe(teams_data)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(\"\\nDataFrame:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Failed to retrieve ODI team rankings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7328e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines from CNBC World News:\n",
      "1. Possible there will be no rate cuts this year, European Central Bank member says\n",
      "2. Goldman Sachs says it's time to buy this unloved global sector — and names some top stocks\n",
      "3. Veteran investor David Roche says the market is getting it wrong on Taiwan's election result\n",
      "4. South Africa's genocide case against Israel has 'global support,' finance minister says\n",
      "5. World's five richest men have doubled their wealth since 2020, report finds\n",
      "6. Two English soccer clubs charged for breaching Premier League profitability and sustainability rules\n",
      "7. Here are Wall Street's top semiconductor stock picks for 2024\n",
      "8. This beach city is helping residents age in place. What to know if you want to stay in your home\n",
      "9. From livestreaming to 'shoppertainment,' Gen Zers lead the way in Singapore's online shopping space\n",
      "10. Ukraine says it has destroyed a Russian spy plane and command aircraft\n",
      "11. Goldman Sachs says these China stocks can win even if there's a repeat of Japan's lost decades\n",
      "12. Top economists predict global growth will weaken this year as geopolitical rifts grow\n",
      "13. Germany skirts recession at the end of 2023 but faces prolonged slump\n",
      "14. Heading to Davos, here's what was overheard on connecting trains from Zurich to WEF\n",
      "15. Apple offers rare iPhone discount in China as demand fears rise\n",
      "16. Japan markets are hitting multi-decade highs — will Nikkei scale a new peak?\n",
      "17. Americans are canceling trips that are thousands of miles from Gaza. Here's why\n",
      "18. China stocks pare losses as central bank holds rates; Taiwan stocks rise after elections\n",
      "19. 'Taiwan is China's Taiwan': Beijing reacts to pivotal presidential election\n",
      "20. Microsoft tops Apple as world's most valuable public company\n",
      "\n",
      "DataFrame:\n",
      "                                             Headline\n",
      "0   Possible there will be no rate cuts this year,...\n",
      "1   Goldman Sachs says it's time to buy this unlov...\n",
      "2   Veteran investor David Roche says the market i...\n",
      "3   South Africa's genocide case against Israel ha...\n",
      "4   World's five richest men have doubled their we...\n",
      "5   Two English soccer clubs charged for breaching...\n",
      "6   Here are Wall Street's top semiconductor stock...\n",
      "7   This beach city is helping residents age in pl...\n",
      "8   From livestreaming to 'shoppertainment,' Gen Z...\n",
      "9   Ukraine says it has destroyed a Russian spy pl...\n",
      "10  Goldman Sachs says these China stocks can win ...\n",
      "11  Top economists predict global growth will weak...\n",
      "12  Germany skirts recession at the end of 2023 bu...\n",
      "13  Heading to Davos, here's what was overheard on...\n",
      "14  Apple offers rare iPhone discount in China as ...\n",
      "15  Japan markets are hitting multi-decade highs —...\n",
      "16  Americans are canceling trips that are thousan...\n",
      "17  China stocks pare losses as central bank holds...\n",
      "18  'Taiwan is China's Taiwan': Beijing reacts to ...\n",
      "19  Microsoft tops Apple as world's most valuable ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_headlines(url):\n",
    "    # Send a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    # Find the container with the headlines\n",
    "    headlines_container = soup.find('div', class_='RiverPlus-riverPlusContainer')\n",
    "    if not headlines_container:\n",
    "        print(\"Unable to find the headlines container.\")\n",
    "        return None\n",
    "\n",
    "    # Extract headlines\n",
    "    headlines = [headline.text.strip() for headline in headlines_container.find_all('div', class_='RiverHeadline-headline RiverHeadline-hasThumbnail')]\n",
    "\n",
    "    return headlines\n",
    "\n",
    "def create_dataframe(headlines):\n",
    "    # Create a DataFrame from the list of headlines\n",
    "    df = pd.DataFrame({'Headline': headlines})\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # URL of the CNBC world news page\n",
    "    cnbc_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "    # Get headlines from the specified URL\n",
    "    headlines = scrape_cnbc_headlines(cnbc_url)\n",
    "   \n",
    "    if headlines:\n",
    "        # Display headlines\n",
    "        print(\"Headlines from CNBC World News:\")\n",
    "        for i, headline in enumerate(headlines, start=1):\n",
    "            print(f\"{i}. {headline}\")\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = create_dataframe(headlines)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(\"\\nDataFrame:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Failed to retrieve headlines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "29c16f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines from CNBC World News:\n",
      "1. 1 Hour Ago7 simple steps to 'strategize your life' and set yourself up for success in 2024\n",
      "2. 2 Hours Ago'Very good chance' that U.S. passes stablecoin laws this year, Circle CEO says\n",
      "3. 3 Hours Ago42-year-old makes more than $1 million/yr on newsletter, podcast—his best advice\n",
      "4. 3 Hours AgoU.S.-owned vessel struck by a ballistic missile off the coast of Yemen\n",
      "5. 4 Hours AgoChina needs reforms to halt 'significant' growth declines, IMF chief says\n",
      "6. 4 Hours AgoDanish happiness and hygge expert: How to stay motivated in the dead of winter\n",
      "7. 4 Hours AgoWhat to know about financial advice as policymakers debate changes to the rules\n",
      "8. 4 Hours Ago5 'red flags' you're dealing with a narcissistic boss—they're the 'worst type,' says career expert\n",
      "9. 4 Hours AgoHow value investor Bill Nygren made 80 times his money buying a media company\n",
      "10. 4 Hours AgoSouth Africa's genocide case against Israel has 'global support,' finance minister says\n",
      "11. 4 Hours AgoInflation is still a mixed bag, but markets don't care. They see rate cuts ahead\n",
      "12. 4 Hours AgoEarnings playbook: Your guide to trading this week's biggest reports\n",
      "13. 4 Hours AgoHow saving $25 a month helped this ‘cash-strapped’ single mom start a business\n",
      "14. 4 Hours AgoHow much money you need to retire in every U.S. state\n",
      "15. 4 Hours AgoYou should replace skincare, makeup products more often than you think\n",
      "16. 5 Hours AgoHow to find a reasonably-priced and resilient stock during earnings season\n",
      "17. 5 Hours AgoHere are 3 ways Gen Zers can build credit before renting their own place\n",
      "18. 6 Hours AgoPossible there will be no rate cuts this year, European Central Bank member says\n",
      "19. 6 Hours AgoVeteran investor says the market is getting it wrong on Taiwan's election result\n",
      "20. 6 Hours Ago10 years in, GM CEO Barra has built her legacy on change and crisis\n",
      "21. 7 Hours AgoWorld's five richest men have doubled their fortunes since 2020: Oxfam report\n",
      "22. 8 Hours AgoIMF warns AI to hit almost 40% of jobs worldwide and worsen overall inequality\n",
      "23. 8 Hours AgoBiden reelection effort raises $97 million in Q4, with $117 million war chest\n",
      "24. 8 Hours AgoEconomists predict global growth will weaken this year as geopolitical rifts grow\n",
      "25. 9 Hours AgoGermany skirts recession at the end of 2023 but faces prolonged slump\n",
      "26. 9 Hours AgoApple offers rare iPhone discount in China as demand fears rise\n",
      "27. 10 Hours AgoUkraine says it has destroyed a Russian spy plane and command aircraft\n",
      "28. 10 Hours AgoHeading to Davos, here's what was overheard on connecting trains from Zurich to WEF\n",
      "29. 11 Hours AgoCNBC Daily Open: Big Bank earnings point to a grim season\n",
      "30. 13 Hours AgoEuropean markets close lower as Davos kicks off; German GDP falls 0.3% in 2023\n",
      "\n",
      "DataFrame:\n",
      "                                             Headline\n",
      "0   1 Hour Ago7 simple steps to 'strategize your l...\n",
      "1   2 Hours Ago'Very good chance' that U.S. passes...\n",
      "2   3 Hours Ago42-year-old makes more than $1 mill...\n",
      "3   3 Hours AgoU.S.-owned vessel struck by a balli...\n",
      "4   4 Hours AgoChina needs reforms to halt 'signif...\n",
      "5   4 Hours AgoDanish happiness and hygge expert: ...\n",
      "6   4 Hours AgoWhat to know about financial advice...\n",
      "7   4 Hours Ago5 'red flags' you're dealing with a...\n",
      "8   4 Hours AgoHow value investor Bill Nygren made...\n",
      "9   4 Hours AgoSouth Africa's genocide case agains...\n",
      "10  4 Hours AgoInflation is still a mixed bag, but...\n",
      "11  4 Hours AgoEarnings playbook: Your guide to tr...\n",
      "12  4 Hours AgoHow saving $25 a month helped this ...\n",
      "13  4 Hours AgoHow much money you need to retire i...\n",
      "14  4 Hours AgoYou should replace skincare, makeup...\n",
      "15  5 Hours AgoHow to find a reasonably-priced and...\n",
      "16  5 Hours AgoHere are 3 ways Gen Zers can build ...\n",
      "17  6 Hours AgoPossible there will be no rate cuts...\n",
      "18  6 Hours AgoVeteran investor says the market is...\n",
      "19  6 Hours Ago10 years in, GM CEO Barra has built...\n",
      "20  7 Hours AgoWorld's five richest men have doubl...\n",
      "21  8 Hours AgoIMF warns AI to hit almost 40% of j...\n",
      "22  8 Hours AgoBiden reelection effort raises $97 ...\n",
      "23  8 Hours AgoEconomists predict global growth wi...\n",
      "24  9 Hours AgoGermany skirts recession at the end...\n",
      "25  9 Hours AgoApple offers rare iPhone discount i...\n",
      "26  10 Hours AgoUkraine says it has destroyed a Ru...\n",
      "27  10 Hours AgoHeading to Davos, here's what was ...\n",
      "28  11 Hours AgoCNBC Daily Open: Big Bank earnings...\n",
      "29  13 Hours AgoEuropean markets close lower as Da...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_headlines(url):\n",
    "    # Send a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    # Find the container with the headlines\n",
    "    headlines_container = soup.find('div', class_='LatestNews-isHomePage LatestNews-isIntlHomepage')\n",
    "\n",
    "    if not headlines_container:\n",
    "        print(\"Unable to find the headlines container.\")\n",
    "        return None\n",
    "\n",
    "    # Extract headlines\n",
    "    headlines = [headline.text.strip() for headline in headlines_container.find_all('div', class_='LatestNews-headlineWrapper')]\n",
    "\n",
    "    return headlines\n",
    "\n",
    "def create_dataframe(headlines):\n",
    "    # Create a DataFrame from the list of headlines\n",
    "    df = pd.DataFrame({'Headline': headlines})\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # URL of the CNBC world news page\n",
    "    cnbc_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "    # Get headlines from the specified URL\n",
    "    headlines = scrape_cnbc_headlines(cnbc_url)\n",
    "   \n",
    "    if headlines:\n",
    "        # Display headlines\n",
    "        print(\"Headlines from CNBC World News:\")\n",
    "        for i, headline in enumerate(headlines, start=1):\n",
    "            print(f\"{i}. {headline}\")\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = create_dataframe(headlines)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(\"\\nDataFrame:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Failed to retrieve headlines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820b42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd80391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4d2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a8f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90100782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646df46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c193e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b3f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058254e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "651f58b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2325225388.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    response = requests.get(\"https://presidentofindia.nic.in/former-presidents\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fdd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fb2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d94d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed5b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7e130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb713341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019ccee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e824d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c53f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee94b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccebf53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47137828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e1a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02835ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69710062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f8037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a778e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40e86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abda6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
